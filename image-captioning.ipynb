{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Captioning Final Project\n\n## Names: Kavya Angara, Saurabh Arora, Parthiv Borgohain, Rudraksh Garg, Pratik Gawli","metadata":{}},{"cell_type":"markdown","source":"### TPU Setup Attempt","metadata":{}},{"cell_type":"code","source":"# !pip install cloud-tpu-client==0.10 torch==1.12.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl\n\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n\n# import os \n# os.environ['LD_LIBRARY_PATH']='/kaggle/working'\n\n# !echo $LD_LIBRARY_PATH\n# !sudo ln -s /usr/local/lib/libmkl_intel_lp64.so /usr/local/lib/libmkl_intel_lp64.so.1\n# !sudo ln -s /usr/local/lib/libmkl_intel_thread.so /usr/local/lib/libmkl_intel_thread.so.1\n# !sudo ln -s /usr/local/lib/libmkl_core.so /usr/local/lib/libmkl_core.so.1\n\n# !ldconfig\n# !ldd /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so\n\n# import torch_xla\n# import torch_xla.core.xla_model as xm","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:39:50.965428Z","iopub.execute_input":"2022-12-07T07:39:50.965898Z","iopub.status.idle":"2022-12-07T07:39:50.986817Z","shell.execute_reply.started":"2022-12-07T07:39:50.965803Z","shell.execute_reply":"2022-12-07T07:39:50.985979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"# Data Libraries\nimport numpy as np\nimport pandas as pd\n\n# Accessing files\nimport os\n\n# Progress display\nfrom tqdm import tqdm\n\n# Sampling data\nimport random\n\n# Plots\nimport matplotlib.pyplot as plt\n\n# Vocab Counter\nfrom collections import Counter\n\n# Display and process images\nfrom PIL import Image\n\n# Deep learning libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# BLEU score calculations\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"id":"UzsrRoRHHcbG","execution":{"iopub.status.busy":"2022-12-07T07:39:51.532255Z","iopub.execute_input":"2022-12-07T07:39:51.532774Z","iopub.status.idle":"2022-12-07T07:39:53.215025Z","shell.execute_reply.started":"2022-12-07T07:39:51.532742Z","shell.execute_reply":"2022-12-07T07:39:53.214085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Google Colab connect to Google Drive files","metadata":{}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"q-WEY4t4HeXL","outputId":"902cf47c-d94e-4f14-a807-77ad95de6f6c","execution":{"iopub.status.busy":"2022-12-07T07:39:53.216947Z","iopub.execute_input":"2022-12-07T07:39:53.217392Z","iopub.status.idle":"2022-12-07T07:39:53.224762Z","shell.execute_reply.started":"2022-12-07T07:39:53.217363Z","shell.execute_reply":"2022-12-07T07:39:53.223754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set up image folders and caption file locations","metadata":{}},{"cell_type":"code","source":"FLICKR_IMAGES_DIR = \"/kaggle/input/flickr8k/Images/\"\nFLICKR_LABEL_PATH = \"/kaggle/input/flickr8k/captions.txt\"\nCUSTOM_IMAGES_DIR = \"/kaggle/input/custom-images/Images/\"\nCUSTOM_LABEL_PATH = \"/kaggle/input/merged-data/custom_csv_all.csv\"\n","metadata":{"id":"7Q6KYGMKHcbG","execution":{"iopub.status.busy":"2022-12-07T07:39:53.228071Z","iopub.execute_input":"2022-12-07T07:39:53.228431Z","iopub.status.idle":"2022-12-07T07:39:53.235451Z","shell.execute_reply.started":"2022-12-07T07:39:53.228404Z","shell.execute_reply":"2022-12-07T07:39:53.234513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Collection","metadata":{}},{"cell_type":"code","source":"# Read custom build crime dataset\ncustom_df = pd.read_csv(CUSTOM_LABEL_PATH, names = [\"image\", \"caption\"])\n# Convert captions to lower case\ncustom_df[\"caption\"] = custom_df.caption.str.lower()\n# Get full paths to the images for each caption\ncustom_df[\"image\"] = CUSTOM_IMAGES_DIR + custom_df[\"image\"]\n# Only keep captions that we have image files for\ncustom_df['exists'] = custom_df['image'].astype(str).map(os.path.exists)\ncustom_df = custom_df[custom_df['exists'] == True][[\"image\", 'caption']]\n# Keep captions that have more than 1 word only\ncustom_df = custom_df[custom_df['caption'].str.split().str.len() > 1]\nprint(custom_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:39:53.790373Z","iopub.execute_input":"2022-12-07T07:39:53.790702Z","iopub.status.idle":"2022-12-07T07:39:54.506011Z","shell.execute_reply.started":"2022-12-07T07:39:53.790676Z","shell.execute_reply":"2022-12-07T07:39:54.504860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read flickr dataset\nflickr_df = pd.read_csv(FLICKR_LABEL_PATH)\n# Convert captions to lower case\nflickr_df[\"caption\"] = flickr_df.caption.str.lower()\n# Remove punctuation from captions\nflickr_df[\"caption\"] = flickr_df.caption.str.replace(\"[^\\w\\s]\",\"\")\n# Remove extra white space from captions\nflickr_df[\"caption\"] = flickr_df.caption.str.strip()\n# Get full paths to the images for each caption\nflickr_df[\"image\"] = FLICKR_IMAGES_DIR + flickr_df[\"image\"]\n# Only keep captions that we have image files for\nflickr_df['exists'] = flickr_df['image'].astype(str).map(os.path.exists)\nflickr_df = flickr_df[flickr_df['exists'] == True][[\"image\", 'caption']]\n# Keep captions that have more than 1 word only\nflickr_df = flickr_df[flickr_df['caption'].str.split().str.len() > 1]\nprint(flickr_df.shape)","metadata":{"id":"3yqf6vo9IcPL","outputId":"aea1b8ab-5c03-4fb0-cdd8-f10f28ed4710","execution":{"iopub.status.busy":"2022-12-07T07:39:54.508068Z","iopub.execute_input":"2022-12-07T07:39:54.508836Z","iopub.status.idle":"2022-12-07T07:40:10.764494Z","shell.execute_reply.started":"2022-12-07T07:39:54.508800Z","shell.execute_reply":"2022-12-07T07:40:10.763458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the crime dataset and the flickr dataset\ndf = pd.concat([flickr_df, custom_df])","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:40:10.766081Z","iopub.execute_input":"2022-12-07T07:40:10.766488Z","iopub.status.idle":"2022-12-07T07:40:10.773971Z","shell.execute_reply.started":"2022-12-07T07:40:10.766450Z","shell.execute_reply":"2022-12-07T07:40:10.773032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pull random caption to confirm cleaning\ndf[\"caption\"].sample(n=1).item()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:40:10.776471Z","iopub.execute_input":"2022-12-07T07:40:10.776920Z","iopub.status.idle":"2022-12-07T07:40:10.789048Z","shell.execute_reply.started":"2022-12-07T07:40:10.776882Z","shell.execute_reply":"2022-12-07T07:40:10.788155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train-Validation-Test Split","metadata":{}},{"cell_type":"code","source":"# Split into train test\ntrain_images = random.sample(list(df[\"image\"].unique()), int(len(df[\"image\"].unique())*.8))\ntrain = df[df[\"image\"].isin(train_images)]\ntest = df[~df[\"image\"].isin(train_images)]","metadata":{"id":"8lozEwhWPk3x","execution":{"iopub.status.busy":"2022-12-07T07:40:10.790635Z","iopub.execute_input":"2022-12-07T07:40:10.791313Z","iopub.status.idle":"2022-12-07T07:40:10.829759Z","shell.execute_reply.started":"2022-12-07T07:40:10.791286Z","shell.execute_reply":"2022-12-07T07:40:10.828940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into train val\ntrain_images = random.sample(list(train[\"image\"].unique()), int(len(train[\"image\"].unique())*.8))\nval = train[~train[\"image\"].isin(train_images)]\ntrain = train[train[\"image\"].isin(train_images)]","metadata":{"id":"3djQHY5CSYX_","execution":{"iopub.status.busy":"2022-12-07T07:40:10.830921Z","iopub.execute_input":"2022-12-07T07:40:10.831643Z","iopub.status.idle":"2022-12-07T07:40:10.868981Z","shell.execute_reply.started":"2022-12-07T07:40:10.831605Z","shell.execute_reply":"2022-12-07T07:40:10.868163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get sizes of each data set\nprint(train.shape)\nprint(val.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:40:10.870359Z","iopub.execute_input":"2022-12-07T07:40:10.870858Z","iopub.status.idle":"2022-12-07T07:40:10.876293Z","shell.execute_reply.started":"2022-12-07T07:40:10.870822Z","shell.execute_reply":"2022-12-07T07:40:10.875366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a custom dataset that holds images and their captions\nclass CustomDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, images, captions, transforms, min_threshold = 5, max_length = 30):\n        \n        self.images = images\n        self.captions = captions\n        self.transform = transforms\n        \n        self.min_threshold = min_threshold\n        self.max_length = max_length\n        \n        self.word_to_idx = None\n        self.idx_to_word = None\n        self.vocab_size = None\n        \n        self._build_vocab()\n        self._tokenize_captions()\n            \n    def __getitem__(self, index):\n        # Get the image path\n        img_path = self.images[index]\n        # Open the image file\n        image = Image.open(self.images[index]).convert(\"RGB\")\n        # Transform the image\n        image = self.transform(image)\n        \n        # Convert tokenized caption to tensor\n        token = torch.as_tensor(self.captions[index])\n        \n        return image, token\n    \n    def __len__(self):\n        return len(self.captions)\n    \n    def _build_vocab(self):\n        # Get the frequency of each word occuring in all captions\n        count_word = dict(Counter(word for sentence in self.captions for word in sentence.split()))\n\n        self.word_to_idx = {}\n        \n        # Prefine a pad and start token\n        self.word_to_idx[\"PAD\"] = 0\n        self.word_to_idx[\"START\"] = 1\n\n        idx = 2\n        \n        # For all captions\n        for caption in self.captions:\n            # get list of words in caption\n            caption_words = caption.split()\n            # for each word\n            for word in caption_words:\n                # if the word occurs more than defined threshold and is not already tokenized\n                if word and count_word[word] >= self.min_threshold and word not in self.word_to_idx:\n                    # Add to vocab as token\n                    self.word_to_idx[word] = idx\n                    idx += 1\n        \n        # Prefine an unknown and end token\n        self.word_to_idx[\"UNKNOWN\"] = idx\n        self.word_to_idx[\"END\"] = idx + 1\n\n        # Create inverse lookup table\n        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n        self.vocab_size = len(self.word_to_idx)\n        \n    def _tokenize_captions(self):\n\n        all_tokens = []\n        # For each caption\n        for caption in self.captions:\n            # Add start token to as first word\n            tokens = [self.word_to_idx[\"START\"]]\n\n            caption_words = caption.split()\n            # For each word\n            for word in caption_words:\n                # need to add end token so if at max length of caption - 1, stop tokenizing\n                if len(tokens) == self.max_length-1:\n                    break\n\n                try:\n                    tokens.append(self.word_to_idx[word])\n                except KeyError:\n                    # word does not exist in vocab, add unknown\n                    tokens.append(self.word_to_idx[\"UNKNOWN\"])\n            \n            # Add end token at the end of the caption\n            tokens.append(self.word_to_idx[\"END\"])\n            \n            # If there is still more space to be filled in to reach max length\n            while len(tokens) < self.max_length:\n                # add a pad token\n                tokens.append(self.word_to_idx[\"PAD\"])\n            \n            all_tokens.append(tokens)\n        \n        it = iter(all_tokens)\n        the_len = len(next(it))\n        if not all(len(l) == the_len for l in it):\n            raise ValueError()\n        self.captions = all_tokens\n\n        ","metadata":{"id":"Rcnhu0mdHcbJ","execution":{"iopub.status.busy":"2022-12-07T07:41:19.582369Z","iopub.execute_input":"2022-12-07T07:41:19.583214Z","iopub.status.idle":"2022-12-07T07:41:19.628982Z","shell.execute_reply.started":"2022-12-07T07:41:19.583169Z","shell.execute_reply":"2022-12-07T07:41:19.627927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Give type of transforms to be done to the image\nimage_transforms = transforms.Compose([ \n    transforms.Resize(256),                          \n    transforms.RandomCrop(224),                      \n    transforms.RandomHorizontalFlip(),               \n    transforms.ToTensor(),                           \n    transforms.Normalize((0.485, 0.456, 0.406),      \n                         (0.229, 0.224, 0.225))])\n\n# Create datasets for train and validation\ntrain_dataset = CustomDataset(list(train[\"image\"]), list(train[\"caption\"]), image_transforms)\nval_dataset = CustomDataset(list(val[\"image\"]), list(val[\"caption\"]), image_transforms)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:41:19.631383Z","iopub.execute_input":"2022-12-07T07:41:19.632298Z","iopub.status.idle":"2022-12-07T07:41:20.504687Z","shell.execute_reply.started":"2022-12-07T07:41:19.632260Z","shell.execute_reply":"2022-12-07T07:41:20.503647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.vocab_size","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:41:22.428905Z","iopub.execute_input":"2022-12-07T07:41:22.429281Z","iopub.status.idle":"2022-12-07T07:41:22.435348Z","shell.execute_reply.started":"2022-12-07T07:41:22.429252Z","shell.execute_reply":"2022-12-07T07:41:22.434356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = train_dataset.max_length\nvocab_size = train_dataset.vocab_size\nbatch_size = 32\nepochs = 10\n","metadata":{"id":"SeS4Bs_fHcbK","execution":{"iopub.status.busy":"2022-12-07T07:41:23.009321Z","iopub.execute_input":"2022-12-07T07:41:23.009644Z","iopub.status.idle":"2022-12-07T07:41:23.014320Z","shell.execute_reply.started":"2022-12-07T07:41:23.009617Z","shell.execute_reply":"2022-12-07T07:41:23.013369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"id":"mBhcZqF2VwIh","outputId":"2b9b1843-ee90-418d-8bd4-453a29deafe8","execution":{"iopub.status.busy":"2022-12-07T07:41:23.720503Z","iopub.execute_input":"2022-12-07T07:41:23.721237Z","iopub.status.idle":"2022-12-07T07:41:23.802181Z","shell.execute_reply.started":"2022-12-07T07:41:23.721201Z","shell.execute_reply":"2022-12-07T07:41:23.801221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Models","metadata":{}},{"cell_type":"code","source":"# Create a CNN model that will define what is in the image\nclass CNN(nn.Module):\n\n    def __init__(self, dropout=0.5, embedding_size = 256):\n        super().__init__()\n        # Using pretrained resnet 50 model\n        self.cnn = torchvision.models.resnet50(pretrained=True)\n        # Dropout to reduce overfitting\n        self.dropout = nn.Dropout(dropout)\n        # Convert to embedding size for LSTM\n        self.linears = nn.Sequential(\n            nn.Linear(1000, embedding_size*2),\n            nn.ReLU(),\n            nn.Linear(embedding_size*2, embedding_size),\n            nn.ReLU()\n        )\n\n    def forward(self, img):\n        return self.linears(self.dropout(self.cnn(img)))\n\n# Create an LSTM model that will generate text to describe image\nclass LSTM(nn.Module):\n\n    def __init__(self, vocab_size, word_to_idx, idx_to_word, embedding_size=256, hidden_size = 256, num_layers = 1, bidirectional = False, dropout=0.5):\n        super().__init__()\n       \n        self.word_to_idx = word_to_idx\n        self.idx_to_word = idx_to_word\n        \n        self.num_layers = num_layers\n        # Embedding layer for looking up vocab and their embedding\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, padding_idx=0)\n        # Main model\n        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n        # Predict next word\n        self.classifier = nn.Sequential(\n            nn.Linear(embedding_size, embedding_size*2),\n            nn.ReLU(),\n            nn.Linear(embedding_size*2, embedding_size*4),\n            nn.ReLU(),\n            nn.Linear(embedding_size*4, embedding_size*8),\n            nn.ReLU(),\n            nn.Linear(embedding_size*8, vocab_size)\n        )\n        # Reduce overfitting\n        self.dropout = nn.Dropout(dropout)\n        # Convert to probabilties\n        self.softmax = nn.Softmax(dim=-1)\n    \n    def forward(self, features, caption):\n        # Output from CNN is input into LSTM\n        features = torch.stack([features]*(self.num_layers), dim=0)\n        # Get the embedding of the caption\n        caption = self.dropout(self.embedding(caption))\n        # Run caption and features through LSTM\n        lstm_out, _ = self.lstm(caption, (features, features))\n        # Return vector of next most likely word values\n        return self.classifier(lstm_out)\n\n    def predict(self, features):\n        # Output from CNN is input into LSTM\n        features = torch.stack([features]*(self.num_layers), dim=0)\n        output = []\n        hidden = (features, features)\n        # Start with start token\n        out = self.word_to_idx[\"START\"]\n        # Until we get to the last word or reach our max length\n        while out != self.word_to_idx[\"END\"] and len(output) <= max_length:\n            out = torch.tensor([[out]]).to(device)\n            # Get the embedding of already constucted portion of the caption\n            out = self.embedding(out)\n            # Pass through LSTM\n            out, hidden = self.lstm(out, hidden)\n            # Get vector of most likely word values\n            out = self.classifier(out)\n            # Convert to probabilties\n            out = self.softmax(out)\n            # Get the most likely word\n            out = torch.argmax(out, dim=-1)\n            # Add most likely word to constucted caption\n            out = out.squeeze().item()\n            output.append(out)\n        \n        # Convert each token to a readable word\n        predicted_caption = [self.idx_to_word[token] for token in output] \n        # Convert to string\n        return \" \".join(predicted_caption)\n","metadata":{"id":"W17h2hagHcbK","execution":{"iopub.status.busy":"2022-12-07T07:41:25.057831Z","iopub.execute_input":"2022-12-07T07:41:25.058217Z","iopub.status.idle":"2022-12-07T07:41:25.077854Z","shell.execute_reply.started":"2022-12-07T07:41:25.058185Z","shell.execute_reply":"2022-12-07T07:41:25.076759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction Function","metadata":{}},{"cell_type":"code","source":"def predict_caption(image_file, data_set, show_actual=False, show_prediction = False, show_image=False):\n    # Open image file\n    input_image = Image.open(image_file).convert(\"RGB\")\n     \n    # Transform image as done in training data\n    input_tensor = image_transforms(input_image).unsqueeze(0)\n    input_tensor = input_tensor.to(device)\n    # Get the predictions from cnn of what is in the image\n    features = cnn(input_tensor)\n    # Generate text using LSTM of image caption\n    pred_caption = lstm.predict(features)\n    \n    # Get the true value of the caption from data\n    actual_caption = data_set[data_set[\"image\"] == image_file][\"caption\"].values\n    \n    # Flags for output\n    if show_prediction:\n        print(\"PREDICTED: \" + \" \".join(pred_caption.split()[:-1]))\n    if show_actual:\n        print(\"ACTUAL: \"+ actual_caption[0])\n    if show_image:\n        display(input_image)\n    \n    # Get words in list for BLEU score calculations\n    actual_caption_words = [i.split() for i in actual_caption]\n    # Return actual and predicted without the end token\n    return actual_caption_words, pred_caption.split()[:-1]","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:41:26.216223Z","iopub.execute_input":"2022-12-07T07:41:26.216577Z","iopub.status.idle":"2022-12-07T07:41:26.224843Z","shell.execute_reply.started":"2022-12-07T07:41:26.216547Z","shell.execute_reply":"2022-12-07T07:41:26.223614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Define CNN and LSTM model\ncnn = CNN(dropout=.2).to(device)\nlstm = LSTM(vocab_size = vocab_size, word_to_idx = train_dataset.word_to_idx, idx_to_word = train_dataset.idx_to_word, dropout=.2).to(device)\n\n# Define learning rate\nlr = 0.001\n\n# Define optimizer and loss function\noptimizer = optim.Adam(lstm.parameters(), lr=lr, weight_decay=1e-6)\nloss_func = nn.CrossEntropyLoss()","metadata":{"id":"0LrIe5cEdXLv","outputId":"d5a1ec93-f61a-4fe1-b0b1-2aac08b03f48","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)","metadata":{"id":"PPMLcyIHeChi","execution":{"iopub.status.busy":"2022-12-07T07:41:58.646251Z","iopub.execute_input":"2022-12-07T07:41:58.646605Z","iopub.status.idle":"2022-12-07T07:41:58.651508Z","shell.execute_reply.started":"2022-12-07T07:41:58.646575Z","shell.execute_reply":"2022-12-07T07:41:58.650551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show number of batches in each dataloader\nlen(train_dataloader), len(val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:41:59.453501Z","iopub.execute_input":"2022-12-07T07:41:59.453827Z","iopub.status.idle":"2022-12-07T07:41:59.460308Z","shell.execute_reply.started":"2022-12-07T07:41:59.453800Z","shell.execute_reply":"2022-12-07T07:41:59.459394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for calculating bleu score for model performance\ndef get_bleu_scores(dataset):\n    # Get all images in dataset\n    dataset_images = dataset['image'].unique()\n    # For each image, get the predicted caption\n    preds = [\" \".join(predict_caption(i,dataset)[1]) for i in dataset_images]\n    \n    pred_df = pd.DataFrame()\n    pred_df[\"image\"] = dataset_images\n    pred_df[\"pred\"] = preds\n    \n    # Create 4 column dataframe with image, prediction, list of actual captions, and bleu score\n    pred_df = pd.merge(pred_df, dataset.groupby(\"image\")['caption'].apply(list), on=\"image\", how=\"left\")\n    \n    pred_df['bleu'] = pred_df.apply(lambda x: sentence_bleu(x.caption, x.pred), axis=1)\n    \n    return pred_df","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:42:00.202028Z","iopub.execute_input":"2022-12-07T07:42:00.203099Z","iopub.status.idle":"2022-12-07T07:42:00.210528Z","shell.execute_reply.started":"2022-12-07T07:42:00.203062Z","shell.execute_reply":"2022-12-07T07:42:00.209216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_every = 100\n\n# Track loss of each batch\ntrain_step_losses = []\nval_step_losses = []\n\n# Track loss of each epoch\ntrain_epoch_losses = []\nval_epoch_losses = []\n\n# Track BLEU of each epoch\ntrain_bleu_scores = []\nval_bleu_scores = []\n\n# Track training BLEU of each batch\ntrain_batch_bleu_scores = []\n\n# Count of number of batches ran\nglobal_train_steps = 0\nglobal_val_steps = 0\n\n# Flag for calculating training BLEU for batches\ncalc_bleu_after_batch = False\n\n# Get random flickr image in training set that will be tested every 100 batches\nstatic_img_1 = train[train[\"image\"].str.contains('flickr8k')].sample(n=1)[\"image\"].values[0]\nstatic_img_2 = train[train[\"image\"].str.contains('flickr8k')].sample(n=1)[\"image\"].values[0]\n\n# Get random crime image in training set that will be tested every 100 batches\nstatic_custom_img = train[train[\"image\"].str.contains('custom-image')].sample(n=1)[\"image\"].values[0]\n\n# Flag for output\nfirst_batch = True\n\n# For each epoch\nfor epoch in range(1, epochs+1):\n    # Create progress bar\n    progress_train = tqdm(train_dataloader)\n    # Track loss\n    total_train_loss = 0\n    # For each batch in training\n    for batch_num, (img, caption) in enumerate(progress_train):\n        # Zero out gradients\n        optimizer.zero_grad()\n        # Do not train cnn\n        cnn.eval()\n        # Train lstm\n        lstm.train()\n        \n        # Send image and caption to cpu or cuda\n        img = img.to(device)\n        caption = caption.to(device)\n        \n        # Get the output of cnn\n        features = cnn(img)\n        # Send output of cnn and all but the last token\n        input_caption = caption[:, :-1]\n        output = lstm(features, input_caption)\n        # Change dimension location\n        output = output.permute(0, 2, 1)\n        # All but first token\n        target = caption[:, 1:]\n        # Get the loss\n        loss = loss_func(output, target)\n        \n        # Back Prop\n        loss.backward() \n    \n        optimizer.step()\n        \n        # For TPU\n        # xm.optimizer_step(optimizer, barrier=True)\n        \n        # Add to progess bar\n        progress_train.set_description(desc=\"Epoch \" + str(epoch) + \" - Train Loss: %.5f\" % (loss.item()))\n        total_train_loss += loss.item()\n        train_step_losses.append(loss.item())\n        global_train_steps += 1\n        \n        # Print exmaples of predictions every 100 batches\n        if (batch_num+1) % print_every == 0:\n            with torch.no_grad():\n                # Do not change weights of LSTM\n                lstm.eval()\n                # Get random image\n                rand_img = train.sample(n=1)[\"image\"].values[0]\n                # Run predictions to see progress\n                predict_caption(static_img_1, train, show_prediction=True, show_image=first_batch, show_actual=first_batch)\n\n                print(\"------------\")\n                predict_caption(static_img_2, train, show_prediction=True,show_image=first_batch, show_actual=first_batch)\n\n                print(\"------------\")\n                predict_caption(static_custom_img, train, show_prediction=True,show_image=first_batch, show_actual=first_batch)\n\n                print(\"------------\")\n                predict_caption(rand_img,train, show_prediction=True, show_image=True, show_actual=True)\n\n                print(\"------------\")\n                if calc_bleu_after_batch:\n                    train_batch_pred_df = get_bleu_scores(train)\n                    avg_train_batch_bleu = train_batch_pred_df[\"bleu\"].mean()\n                    train_batch_bleu_scores.append(avg_train_batch_bleu)\n                    print(\"Avg Training BLEU Score After Batch:\", avg_train_batch_bleu)\n                \n                # Do not display image of static pictures again in batches after the first one\n                first_batch = False\n\n    # Get average loss over epoch\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    train_epoch_losses.append(avg_train_loss)\n    print(\"Avg Training Loss on Epoch \" + str(epoch) + \": \" + str(avg_train_loss))\n    \n    # Validate model performance\n    cnn.eval()\n    lstm.eval()\n    # Get BLEU score of training set\n    print(\"Running in-sample predictions...\")\n    train_pred_df = get_bleu_scores(train)\n    avg_train_bleu = train_pred_df[\"bleu\"].mean()\n    train_bleu_scores.append(avg_train_bleu)\n    print(\"Epoch Avg Training BLEU Score:\", avg_train_bleu)\n    \n    # Similar process of training set\n    print(\"Running Validation on Epoch \"+ str(epoch))\n    progress_val = tqdm(val_dataloader)\n    total_val_loss = 0\n    for batch_num, (img, caption) in enumerate(progress_val):\n        lstm.eval()\n        img = img.to(device)\n        caption = caption.to(device)\n        features = cnn(img)\n        input_caption = caption[:, :-1]\n        output = lstm(features, input_caption)\n        output = output.permute(0, 2, 1)\n        val_loss = loss_func(output, caption[:, 1:])\n            \n        progress_val.set_description(desc=\"Epoch \" + str(epoch) + \" - Val Loss: %.5f\" % (val_loss.item()))\n        \n        total_val_loss += val_loss.item()\n        val_step_losses.append(val_loss.item())\n        global_val_steps += 1\n\n    # Get BLEU score of validation set\n    print(\"Running out-sample val predictions...\")\n    val_pred_df = get_bleu_scores(val)\n    avg_val_bleu = val_pred_df[\"bleu\"].mean()\n    val_bleu_scores.append(avg_val_bleu)\n    print(\"Avg Val BLEU Score:\", avg_val_bleu)\n    \n    # Get random images to see the progress of model at end of epoch\n    rand_img = val.sample(n=1)[\"image\"].values[0]\n    predict_caption(rand_img, val, show_prediction=True, show_image=True, show_actual=True)\n\n    \n    rand_img = val.sample(n=1)[\"image\"].values[0]\n    ref, candiate = predict_caption(rand_img, val, show_prediction=True, show_image=True, show_actual=True)\n\n    \n    avg_val_loss = total_val_loss / len(val_dataloader)\n    val_epoch_losses.append(avg_val_loss)\n\n    print(\"Avg Val Loss on Epoch \" + str(epoch) + \": \" + str(avg_val_loss))\n    \n    # Print model performance graphs...was not able to run tensorboard on Kaggle\n    plt.plot(list(range(global_train_steps)),train_step_losses)\n    plt.xlabel('training batches')\n    plt.ylabel('train loss')\n    plt.title(\"Batch Training Loss\")\n    plt.show()\n    plt.clf()\n    \n    if calc_bleu_after_batch:\n        plt.plot(list(range(global_train_steps)),train_batch_bleu_scores)\n        plt.xlabel('training batches')\n        plt.ylabel('BLEU score')\n        plt.title(\"Avg Train BLEU\")\n        plt.show()\n        plt.clf()\n    \n    \n    plt.plot(list(range(global_val_steps)),val_step_losses)\n    plt.xlabel('val batches')\n    plt.ylabel('val loss')\n    plt.title(\"Batch Val Loss\")\n    plt.show()\n    plt.clf()\n    \n    plt.plot(list(range(1,epoch+1)),train_epoch_losses, label='avg training loss')\n    plt.plot(list(range(1,epoch+1)),val_epoch_losses, label='avg val loss')\n    plt.xlabel('number of epochs')\n    plt.ylabel('loss')\n    plt.title(\"Train v Val Loss\")\n    plt.legend()\n    plt.show()\n    plt.clf()\n\n    plt.plot(list(range(1,epoch+1)),train_bleu_scores, label='avg training bleu')\n    plt.plot(list(range(1,epoch+1)),val_bleu_scores, label='avg val bleu')\n    plt.xlabel('number of epochs')\n    plt.ylabel('BLEU')\n    plt.title(\"Avg Train v Val BLEU\")\n    plt.legend()\n    plt.show()\n    plt.clf()\n    \n    ","metadata":{"id":"Q32FIB_qbhaV","outputId":"7ee80c69-9de6-4145-ddff-425f50ac22ba","execution":{"iopub.status.busy":"2022-12-07T04:22:05.018465Z","iopub.execute_input":"2022-12-07T04:22:05.018855Z","iopub.status.idle":"2022-12-07T06:20:50.922375Z","shell.execute_reply.started":"2022-12-07T04:22:05.018820Z","shell.execute_reply":"2022-12-07T06:20:50.921326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the models\ntorch.save(cnn, '/kaggle/working/cnn.pkl')\ntorch.save(lstm, '/kaggle/working/lstm.pkl')\n","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:21:49.521214Z","iopub.execute_input":"2022-12-07T06:21:49.521573Z","iopub.status.idle":"2022-12-07T06:21:49.796454Z","shell.execute_reply.started":"2022-12-07T06:21:49.521542Z","shell.execute_reply":"2022-12-07T06:21:49.795359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results","metadata":{}},{"cell_type":"code","source":"# Load in saved models\ncnn_temp = torch.load(\"/kaggle/input/final-cnn-lstm-models/cnn (2).pkl\")\nlstm_temp = torch.load('/kaggle/input/final-cnn-lstm-models/lstm (2).pkl')","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:30:10.481469Z","iopub.execute_input":"2022-12-07T06:30:10.481822Z","iopub.status.idle":"2022-12-07T06:30:13.071854Z","shell.execute_reply.started":"2022-12-07T06:30:10.481792Z","shell.execute_reply":"2022-12-07T06:30:13.070710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Do not train models\ncnn.eval()\nlstm.eval()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:24:35.845685Z","iopub.execute_input":"2022-12-07T06:24:35.846180Z","iopub.status.idle":"2022-12-07T06:24:35.853619Z","shell.execute_reply.started":"2022-12-07T06:24:35.846146Z","shell.execute_reply":"2022-12-07T06:24:35.852368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the predictions results for each dataset\ntrain_pred_df = get_bleu_scores(train)\nval_pred_df = get_bleu_scores(val)\ntest_pred_df = get_bleu_scores(test)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:24:38.402023Z","iopub.execute_input":"2022-12-07T06:24:38.402726Z","iopub.status.idle":"2022-12-07T06:29:03.622985Z","shell.execute_reply.started":"2022-12-07T06:24:38.402682Z","shell.execute_reply":"2022-12-07T06:29:03.621973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize results with random image\ndef pull_random_pred(pred_df, crime=True,min_bleu_thresh = 0.0, max_bleu_thresh = 1.0):\n    # If we want to look at custom crime data only\n    if crime:\n        pull_df = pred_df[pred_df[\"image\"].str.contains(\"custom-images\")]\n    else:\n        pull_df = pred_df.copy()\n    \n    # Get desired bleu score for analysis\n    pull_df = pull_df[(pull_df[\"bleu\"] >= min_bleu_thresh)&(pull_df[\"bleu\"] <= max_bleu_thresh)]\n    # Get random record\n    row = pull_df.sample(n=1)\n    \n    # Display output\n    print(\"file:\", row[\"image\"].item())\n    display(Image.open(row[\"image\"].item()))\n    print(\"prediction:\",row[\"pred\"].item())\n    print(\"actuals:\", list(row[\"caption\"]))\n    print(\"BLEU Score\", row[\"bleu\"].item())\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:40:17.027915Z","iopub.execute_input":"2022-12-07T06:40:17.028296Z","iopub.status.idle":"2022-12-07T06:40:17.036441Z","shell.execute_reply.started":"2022-12-07T06:40:17.028266Z","shell.execute_reply":"2022-12-07T06:40:17.035326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pull_random_pred(test_pred_df, crime=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:33:45.426684Z","iopub.execute_input":"2022-12-07T06:33:45.427296Z","iopub.status.idle":"2022-12-07T06:33:45.458664Z","shell.execute_reply.started":"2022-12-07T06:33:45.427257Z","shell.execute_reply":"2022-12-07T06:33:45.457609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pull_random_pred(train_pred_df, crime=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:33:13.856012Z","iopub.execute_input":"2022-12-07T06:33:13.856773Z","iopub.status.idle":"2022-12-07T06:33:13.928668Z","shell.execute_reply.started":"2022-12-07T06:33:13.856733Z","shell.execute_reply":"2022-12-07T06:33:13.927767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pull_random_pred(val_pred_df, crime=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:32:57.736668Z","iopub.execute_input":"2022-12-07T06:32:57.737047Z","iopub.status.idle":"2022-12-07T06:32:57.830636Z","shell.execute_reply.started":"2022-12-07T06:32:57.737016Z","shell.execute_reply":"2022-12-07T06:32:57.829749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of train BLEU scores\ntrain_pred_df.bleu.hist(bins=10)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:34:25.868042Z","iopub.execute_input":"2022-12-07T06:34:25.868402Z","iopub.status.idle":"2022-12-07T06:34:26.183251Z","shell.execute_reply.started":"2022-12-07T06:34:25.868370Z","shell.execute_reply":"2022-12-07T06:34:26.182274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stats of train BLEU scores\ntrain_pred_df.bleu.describe()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:34:27.806917Z","iopub.execute_input":"2022-12-07T06:34:27.807846Z","iopub.status.idle":"2022-12-07T06:34:27.821487Z","shell.execute_reply.started":"2022-12-07T06:34:27.807810Z","shell.execute_reply":"2022-12-07T06:34:27.819906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of val BLEU scores\nval_pred_df.bleu.hist(bins=10)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:34:30.024394Z","iopub.execute_input":"2022-12-07T06:34:30.024900Z","iopub.status.idle":"2022-12-07T06:34:30.255070Z","shell.execute_reply.started":"2022-12-07T06:34:30.024864Z","shell.execute_reply":"2022-12-07T06:34:30.254154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stats of val BLEU scores\nval_pred_df.bleu.describe()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:34:31.205370Z","iopub.execute_input":"2022-12-07T06:34:31.205761Z","iopub.status.idle":"2022-12-07T06:34:31.219291Z","shell.execute_reply.started":"2022-12-07T06:34:31.205727Z","shell.execute_reply":"2022-12-07T06:34:31.217723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of test BLEU scores\ntest_pred_df.bleu.hist(bins=10)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:34:32.972671Z","iopub.execute_input":"2022-12-07T06:34:32.973408Z","iopub.status.idle":"2022-12-07T06:34:33.210853Z","shell.execute_reply.started":"2022-12-07T06:34:32.973366Z","shell.execute_reply":"2022-12-07T06:34:33.209836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stats of test BLEU scores\ntest_pred_df.bleu.describe()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:34:36.413314Z","iopub.execute_input":"2022-12-07T06:34:36.415221Z","iopub.status.idle":"2022-12-07T06:34:36.428905Z","shell.execute_reply.started":"2022-12-07T06:34:36.415180Z","shell.execute_reply":"2022-12-07T06:34:36.427340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bad Test Captions\nfor _ in range(5):\n    pull_random_pred(test_pred_df, crime=False, min_bleu_thresh = 0.0, max_bleu_thresh=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:40:34.509254Z","iopub.execute_input":"2022-12-07T06:40:34.509627Z","iopub.status.idle":"2022-12-07T06:40:34.857852Z","shell.execute_reply.started":"2022-12-07T06:40:34.509577Z","shell.execute_reply":"2022-12-07T06:40:34.856892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Descent Test Captions\nfor _ in range(5):\n    pull_random_pred(test_pred_df, crime=False, min_bleu_thresh = 0.3, max_bleu_thresh=0.7)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:41:50.637981Z","iopub.execute_input":"2022-12-07T06:41:50.638332Z","iopub.status.idle":"2022-12-07T06:41:51.044942Z","shell.execute_reply.started":"2022-12-07T06:41:50.638303Z","shell.execute_reply":"2022-12-07T06:41:51.043901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Good Test Captions\nfor _ in range(5):\n    pull_random_pred(test_pred_df, crime=False, min_bleu_thresh = 0.7, max_bleu_thresh=1.0)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:47:06.891683Z","iopub.execute_input":"2022-12-07T06:47:06.892112Z","iopub.status.idle":"2022-12-07T06:47:07.329780Z","shell.execute_reply.started":"2022-12-07T06:47:06.892077Z","shell.execute_reply":"2022-12-07T06:47:07.328815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Crime Test Captions\nfor _ in range(5):\n    pull_random_pred(test_pred_df, crime=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T06:50:01.939841Z","iopub.execute_input":"2022-12-07T06:50:01.940573Z","iopub.status.idle":"2022-12-07T06:50:03.844025Z","shell.execute_reply.started":"2022-12-07T06:50:01.940538Z","shell.execute_reply":"2022-12-07T06:50:03.843001Z"},"trusted":true},"execution_count":null,"outputs":[]}]}